{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b029e41-1294-4798-b0ce-9be97145f359",
   "metadata": {},
   "source": [
    "This notebook demonstrates use of the TorchRF DataLogger.  The DataLogger allows a user to record intermediate results from a computation (for example, in Sionna) and compare those to intermediate results in a separate computation (for example, in TorchRF).  The calls to save out the data need to be made where the data is available, which might be inside of a function which could be called multiple times.  In order to align saved output, a sequence of tags similar to the call stack is used to name the saved variables.\n",
    "\n",
    "In the basic use case, two separate runners are used for the two separate runs.  The first runner should save outputs and the second runner should load the saved outputs and compare them to newly generated ones.  We demonstrate this in two notebooks.  The current notebook, `LogDemoA`, demonstrates writing.  The second notebook, `LogDemoB`, demonstrates loading what this notebook writes and comparing those reference values with the values computed in B.\n",
    "\n",
    "Start with the beginning of the Sionna demo so we have some data to save."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd2458b-3287-403d-9c1b-048e9768cc00",
   "metadata": {},
   "source": [
    "# Part I: Compute and store Reference values.\n",
    "Start with the beginning of the Sionna demo so we have some data to save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a8209b4-e7ae-4964-9315-3cc22f939e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 21:50:24.420444: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-12 21:50:29.403128: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# The imports below will generate an error unless you set this environment variable in your shell prior to launching jupyter lab:\n",
    "# export DRJIT_LIBLLVM_PATH=/absolute/path/to/libLLVM.dylib\n",
    "import numpy as np\n",
    "import sionna\n",
    "from sionna.rt.scene import load_scene\n",
    "from sionna.rt import PlanarArray, Transmitter, Receiver, Camera\n",
    "# They may also generate Tensorflow configuration warnings that can be ignored.\n",
    "\n",
    "scene = load_scene(sionna.rt.scene.munich)\n",
    "resolution = [480, 320]\n",
    "my_cam = Camera(\"my_cam\", position=[-250, 250, 150], look_at=[-15, 30, 28])\n",
    "scene.add(my_cam)\n",
    "scene.tx_array = PlanarArray(num_rows=1, num_cols=1, vertical_spacing=0.5, horizontal_spacing=0.5,\n",
    "                             pattern=\"tr38901\",polarization=\"V\")\n",
    "tx = Transmitter(name=\"tx\", position=[8.5, 21, 27])\n",
    "scene.add(tx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f6f2b9-1cb0-49b5-90d5-0909edbf03cb",
   "metadata": {},
   "source": [
    "Import the DataLogger and set the mode to 'write'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0841e77c-a45d-4de8-aa6e-cbdc9ad16b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torchrf.utils.datalogger.DataLogger at 0x1044d9a50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchrf.utils.datalogger import DataLogger\n",
    "DataLogger().set_mode('write')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76866c7a-38cb-4b12-95c7-a032befc694e",
   "metadata": {},
   "source": [
    "The DataLogger is a singleton class.  The instance can be obtained as if by a typical constructor, but the same instance is returned every time. This allows the logger to maintain state without needing to be passed into function calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31c99271-9176-417c-ab3e-16706b092929",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = DataLogger()\n",
    "logger2 = DataLogger()\n",
    "assert logger is logger2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8941c5ad-d561-4957-825d-e328cf1d42cd",
   "metadata": {},
   "source": [
    "The simplest way to save out a variable is just to use the write command, which gets a name used for alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4c6803d-4054-48a2-81cf-e8b0282ffab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/log_1705125035.pkl')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.write(tx, 'transmitter1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3b4ee7-e17d-40f1-83ec-6e48d68aa607",
   "metadata": {},
   "source": [
    "This write command pickled the transmitter.  It returns the path to the written file.  The default location is in the directory `logs` under the current working directory, and the names are automatically generated based on timestamps. We don't need to keep track of these file paths because they will be stored in an index at the end.  The default index file is 'logs/index.json'.\n",
    "\n",
    "Tensorflow tensors and PyTorch tensors are converted to numpy.ndarrays before pickling.  The logger recurs through lists, dicts, and complex classes such as Transmmitter.  Because many of the classes we want to save point to Scene objects, which are large, the DataLogger ignores Scene objects.  There is nothing to stop the recursion from traversing down an infinite loop if two objects point to each other, but this does not come up when Scenes are ignored.\n",
    "\n",
    "* FUTURE: track objects to avoid looping.\n",
    "* FUTURE: make it easy for user to specify additional classes to ignore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b15b19-000a-40b8-bd1b-3e2169ee750a",
   "metadata": {},
   "source": [
    "When the user believes that two tensors are identical up to reordering of vectors and that vectors are unique (as happens with the outputs of `tf.raw.uniqueV2` and `torch.unique`, the user can specify a \"unique\" keyword when comparing.  We will demonstrate this on the tensor below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6299ce17-c175-4480-8de7-49b7b25a150e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('logs/log_1705125034_7093.pkl')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([[7, 5, 3],\n",
    "              [7, 5, 3],\n",
    "              [7, 5, 3]])\n",
    "logger.write(a, '753array')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25afda6f-a69f-4bdd-99d7-5f1a9bd5f387",
   "metadata": {},
   "source": [
    "Write calls inside of functions might be called in different contexts, and we will need to track those contexts the same way one tracks the call stack, so that we compare the correct outputs.  The following example throws an error because it tries to save 3 different values under the same tag: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc97330-eed2-4568-97cc-5ecce17a1db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(x):\n",
    "    y = x * x\n",
    "    logger.write(y, 'intermediate foo')\n",
    "    z = y/2\n",
    "    return z\n",
    "\n",
    "z1 = foo(7)\n",
    "error = False\n",
    "try:\n",
    "    z2 = foo(5)\n",
    "except RuntimeError:\n",
    "    error = True\n",
    "assert error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101e0b3-4a35-4f62-9a4a-ab464706313b",
   "metadata": {},
   "source": [
    "We accommodate this by stacking tags with push() and pop().  These tags maintain a prefix that will be prepended on subsequent write calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c4a5cb-c141-47d0-ae4b-c3d9b3c2b084",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.push('foo 1')\n",
    "z1 = foo(7)\n",
    "logger.pop()\n",
    "\n",
    "logger.push('foo 2')\n",
    "z2 = foo(5)\n",
    "logger.pop()\n",
    "\n",
    "logger.push('foo 3')\n",
    "z3 = foo(3)\n",
    "logger.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3b9dce-c62d-4683-92cd-251f59683102",
   "metadata": {},
   "source": [
    "This is made slightly more convenient by using `with` blocks.  The end of `with` block automatically pops the logger and automatically records the index to disk if the stack is empty.  The example below is nested, which will create a different tag structure from the tags above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30276e50-e0c0-4275-8378-f1cdb19e0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DataLogger().push('foo A') as logger:  # Can't use 'foo 1' because it was used above.\n",
    "    z1 = foo(7)\n",
    "    logger.write(z1, 'first returned z')\n",
    "    with logger.push('foo 2') as logger:  # This tag is now 'foo A:foo 2'\n",
    "        z2 = foo(5)\n",
    "        with logger.push('foo 3') as logger:  # This tag is now 'foo A:foo 2:foo 3'\n",
    "            z3 = foo(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9595561a-77bd-4d54-8762-8e35374b9fad",
   "metadata": {},
   "source": [
    "Let's try some data that we know will fail to be recreated in the second run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "346bd24c-12a7-4ab1-9eb5-b930759bc46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with DataLogger().push('rand') as logger:\n",
    "    a = np.random.rand(3, 5)\n",
    "    logger.write(a, 'A')\n",
    "    z = foo(a)\n",
    "    logger.write(z, 'foo A')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62079ea4-7b20-4828-a286-2dbf6b556838",
   "metadata": {},
   "source": [
    "When the with block exits, if the stack is empty then it automatically saves the index.  So ideally you wrap all of your writes in an outermost with block in the runner, as above.  If you have a `write()` that was not in a `with`, then you'll have to call `save()` to save out the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a1ea8e53-7cdf-4b2c-9842-277cfa16f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb911923-4758-45d8-a9a3-c44e88fcdc6c",
   "metadata": {},
   "source": [
    "In this case we did not need to call save this final time, but it only has the effect of overwriting the same information into the index file, so it does no harm.\n",
    "\n",
    "Now run LogDemoB for Part 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
